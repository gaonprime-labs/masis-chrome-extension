# ë¡œì»¬ CLIP ì„œë²„ êµ¬ì¶• ê°€ì´ë“œ

ë¬´ë£Œë¡œ CLIP ì„ë² ë”© ì„œë²„ë¥¼ ì§ì ‘ í˜¸ìŠ¤íŒ…í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­](#ì‹œìŠ¤í…œ-ìš”êµ¬ì‚¬í•­)
2. [Python í™˜ê²½ ì„¤ì •](#python-í™˜ê²½-ì„¤ì •)
3. [CLIP ì„œë²„ êµ¬ì¶•](#clip-ì„œë²„-êµ¬ì¶•)
4. [Next.js í†µí•©](#nextjs-í†µí•©)
5. [ë°°í¬ ì˜µì…˜](#ë°°í¬-ì˜µì…˜)

---

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ìµœì†Œ ì‚¬ì–‘
- **CPU**: 4ì½”ì–´ ì´ìƒ
- **RAM**: 8GB ì´ìƒ
- **Storage**: 5GB (ëª¨ë¸ ìºì‹œ)
- **Python**: 3.9 ì´ìƒ

### ê¶Œì¥ ì‚¬ì–‘ (GPU ì‚¬ìš©)
- **GPU**: NVIDIA GPU (CUDA ì§€ì›)
- **VRAM**: 4GB ì´ìƒ
- **CUDA**: 11.8 ì´ìƒ
- **cudNN**: 8.x

### ì„±ëŠ¥ ë¹„êµ
| í™˜ê²½ | ì´ë¯¸ì§€ë‹¹ ì²˜ë¦¬ ì‹œê°„ | ë°°ì¹˜ ì²˜ë¦¬ (10ì¥) |
|------|------------------|-----------------|
| CPU (4ì½”ì–´) | ~2-3ì´ˆ | ~20ì´ˆ |
| GPU (RTX 3060) | ~0.1ì´ˆ | ~0.5ì´ˆ |
| GPU (RTX 4090) | ~0.05ì´ˆ | ~0.2ì´ˆ |

---

## Python í™˜ê²½ ì„¤ì •

### 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±

```bash
cd /Users/MooSaeng/coding/gaon/character-generator
mkdir clip-server
cd clip-server
```

### 2. ê°€ìƒí™˜ê²½ ìƒì„±

```bash
# Python venv ì‚¬ìš©
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# ë˜ëŠ” conda ì‚¬ìš©
conda create -n clip-server python=3.10
conda activate clip-server
```

### 3. ì˜ì¡´ì„± ì„¤ì¹˜

**CPU ë²„ì „** (macOS/ì¼ë°˜ ì„œë²„):
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install transformers pillow fastapi uvicorn pydantic python-multipart
```

**GPU ë²„ì „** (NVIDIA GPU):
```bash
# CUDA 11.8
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

pip install transformers pillow fastapi uvicorn pydantic python-multipart
```

**Apple Silicon (M1/M2/M3)** - MPS ê°€ì†:
```bash
pip install torch torchvision
pip install transformers pillow fastapi uvicorn pydantic python-multipart
```

---

## CLIP ì„œë²„ êµ¬ì¶•

### 1. ì„œë²„ ì½”ë“œ ì‘ì„±

`clip-server/server.py`:

```python
# clip-server/server.py
import torch
from transformers import CLIPProcessor, CLIPModel
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from PIL import Image
import requests
from io import BytesIO
from typing import List, Literal
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(title="CLIP Embedding Server", version="1.0.0")

# CORS ì„¤ì • (Next.jsì—ì„œ ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "https://ark.gaonprime.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU > MPS > CPU)
if torch.cuda.is_available():
    device = "cuda"
    logger.info("ğŸš€ Using NVIDIA GPU (CUDA)")
elif torch.backends.mps.is_available():
    device = "mps"
    logger.info("ğŸš€ Using Apple Silicon GPU (MPS)")
else:
    device = "cpu"
    logger.warning("âš ï¸  Using CPU (slow performance)")

# CLIP ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
MODEL_NAME = "openai/clip-vit-large-patch14"  # ViT-L/14
logger.info(f"ğŸ“¦ Loading CLIP model: {MODEL_NAME}")

model = CLIPModel.from_pretrained(MODEL_NAME).to(device)
processor = CLIPProcessor.from_pretrained(MODEL_NAME)

logger.info("âœ… CLIP model loaded successfully")

# ìš”ì²­ ìŠ¤í‚¤ë§ˆ
class EmbeddingRequest(BaseModel):
    input: str
    type: Literal["text", "image"]

class BatchEmbeddingRequest(BaseModel):
    inputs: List[str]
    type: Literal["text", "image"]

# ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
class EmbeddingResponse(BaseModel):
    success: bool
    embedding: List[float] | None = None
    error: str | None = None

class BatchEmbeddingResponse(BaseModel):
    success: bool
    embeddings: List[List[float]] | None = None
    error: str | None = None

# í—¬ìŠ¤ì²´í¬
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "device": device,
        "model": MODEL_NAME,
    }

# ë‹¨ì¼ ì„ë² ë”© ìƒì„±
@app.post("/embed", response_model=EmbeddingResponse)
async def create_embedding(request: EmbeddingRequest):
    try:
        logger.info(f"ğŸ” Processing {request.type} embedding request")

        if request.type == "text":
            # í…ìŠ¤íŠ¸ ì„ë² ë”©
            inputs = processor(text=[request.input], return_tensors="pt", padding=True).to(device)

            with torch.no_grad():
                text_features = model.get_text_features(**inputs)
                # L2 ì •ê·œí™”
                text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)

            embedding = text_features[0].cpu().numpy().tolist()

        elif request.type == "image":
            # ì´ë¯¸ì§€ URLì—ì„œ ë‹¤ìš´ë¡œë“œ
            response = requests.get(request.input, timeout=10)
            response.raise_for_status()

            image = Image.open(BytesIO(response.content)).convert("RGB")

            # ì´ë¯¸ì§€ ì„ë² ë”©
            inputs = processor(images=image, return_tensors="pt").to(device)

            with torch.no_grad():
                image_features = model.get_image_features(**inputs)
                # L2 ì •ê·œí™”
                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

            embedding = image_features[0].cpu().numpy().tolist()

        else:
            raise HTTPException(status_code=400, detail="Invalid type")

        logger.info(f"âœ… Embedding created: {len(embedding)} dimensions")

        return EmbeddingResponse(
            success=True,
            embedding=embedding
        )

    except requests.RequestException as e:
        logger.error(f"âŒ Image download failed: {e}")
        return EmbeddingResponse(
            success=False,
            error=f"Failed to download image: {str(e)}"
        )
    except Exception as e:
        logger.error(f"âŒ Embedding failed: {e}")
        return EmbeddingResponse(
            success=False,
            error=f"Embedding generation failed: {str(e)}"
        )

# ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
@app.post("/embed/batch", response_model=BatchEmbeddingResponse)
async def create_batch_embeddings(request: BatchEmbeddingRequest):
    try:
        logger.info(f"ğŸ” Processing batch of {len(request.inputs)} {request.type} embeddings")

        if request.type == "text":
            # ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
            inputs = processor(text=request.inputs, return_tensors="pt", padding=True).to(device)

            with torch.no_grad():
                text_features = model.get_text_features(**inputs)
                text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)

            embeddings = text_features.cpu().numpy().tolist()

        elif request.type == "image":
            # ë°°ì¹˜ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            images = []
            for url in request.inputs:
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                image = Image.open(BytesIO(response.content)).convert("RGB")
                images.append(image)

            # ë°°ì¹˜ ì´ë¯¸ì§€ ì„ë² ë”©
            inputs = processor(images=images, return_tensors="pt").to(device)

            with torch.no_grad():
                image_features = model.get_image_features(**inputs)
                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

            embeddings = image_features.cpu().numpy().tolist()

        else:
            raise HTTPException(status_code=400, detail="Invalid type")

        logger.info(f"âœ… Batch embeddings created: {len(embeddings)} items")

        return BatchEmbeddingResponse(
            success=True,
            embeddings=embeddings
        )

    except Exception as e:
        logger.error(f"âŒ Batch embedding failed: {e}")
        return BatchEmbeddingResponse(
            success=False,
            error=f"Batch embedding generation failed: {str(e)}"
        )

if __name__ == "__main__":
    import uvicorn

    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        app,
        host="0.0.0.0",  # ì™¸ë¶€ ì ‘ê·¼ í—ˆìš©
        port=8000,
        log_level="info"
    )
```

### 2. ì„œë²„ ì‹¤í–‰

```bash
cd clip-server
source venv/bin/activate  # ê°€ìƒí™˜ê²½ í™œì„±í™”

# ê°œë°œ ëª¨ë“œ
python server.py

# ë˜ëŠ” uvicorn ì§ì ‘ ì‚¬ìš©
uvicorn server:app --host 0.0.0.0 --port 8000 --reload
```

### 3. í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health

# í…ìŠ¤íŠ¸ ì„ë² ë”©
curl -X POST http://localhost:8000/embed \
  -H "Content-Type: application/json" \
  -d '{"input": "a happy smiling girl", "type": "text"}'

# ì´ë¯¸ì§€ ì„ë² ë”©
curl -X POST http://localhost:8000/embed \
  -H "Content-Type: application/json" \
  -d '{"input": "https://example.com/image.jpg", "type": "image"}'
```

---

## Next.js í†µí•©

### 1. Local CLIP Client ì¶”ê°€

`src/lib/clip/client.ts`ì— LocalClipClient ì¶”ê°€:

```typescript
/**
 * Local CLIP í´ë¼ì´ì–¸íŠ¸
 *
 * ë¡œì»¬ ì„œë²„ì—ì„œ ì‹¤í–‰ë˜ëŠ” CLIP ëª¨ë¸ ì‚¬ìš©
 */
export class LocalClipClient implements IClipClient {
  private readonly endpoint: string;
  private readonly timeout: number;

  constructor(config: ClipConfig) {
    this.endpoint = config.endpoint || 'http://localhost:8000';
    this.timeout = config.timeout || 30000;
  }

  async getEmbedding(request: EmbeddingRequest): Promise<EmbeddingResponse> {
    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), this.timeout);

      const response = await fetch(`${this.endpoint}/embed`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        signal: controller.signal,
        body: JSON.stringify({
          input: request.input,
          type: request.type,
        }),
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        const error = await response.text();
        return {
          success: false,
          error: `Local CLIP API error: ${response.status} - ${error}`,
        };
      }

      const data = await response.json();

      return {
        success: data.success,
        embedding: data.embedding,
        error: data.error,
      };
    } catch (error) {
      const message = error instanceof Error ? error.message : 'Unknown error';
      return {
        success: false,
        error: `Local CLIP embedding failed: ${message}`,
      };
    }
  }
}
```

### 2. Factory í•¨ìˆ˜ ìˆ˜ì •

```typescript
export function createClipClient(
  config: ClipConfig,
  provider: 'replicate' | 'openai' | 'local' = 'local'
): IClipClient {
  switch (provider) {
    case 'local':
      return new LocalClipClient(config);
    case 'replicate':
      return new ReplicateClipClient(config);
    case 'openai':
      return new OpenAIClipClient(config);
    default:
      throw new Error(`Unknown CLIP provider: ${provider}`);
  }
}
```

### 3. í™˜ê²½ë³€ìˆ˜ ì„¤ì •

`.env.local`:
```bash
# Local CLIP Server
CLIP_PROVIDER=local
LOCAL_CLIP_ENDPOINT=http://localhost:8000

# ReplicateëŠ” ì£¼ì„ ì²˜ë¦¬ (ì‚¬ìš© ì•ˆ í•¨)
# REPLICATE_API_KEY=r8_xxx
```

### 4. unified-select API ìˆ˜ì •

```typescript
// Stage 2.5ì—ì„œ ë¡œì»¬ CLIP ì‚¬ìš©
const clipProvider = process.env.CLIP_PROVIDER || 'local';
const clipEndpoint = process.env.LOCAL_CLIP_ENDPOINT || 'http://localhost:8000';

const clipFilter = new ClipFilter(
  {
    apiKey: '', // ë¡œì»¬ì€ API í‚¤ ë¶ˆí•„ìš”
    endpoint: clipEndpoint,
    timeout: 60000
  },
  clipProvider as 'local' | 'replicate' | 'openai'
);
```

---

## ë°°í¬ ì˜µì…˜

### Option 1: ê°™ì€ ì„œë²„ì—ì„œ ì‹¤í–‰ (ê¶Œì¥)

**ì¥ì **: ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì—†ìŒ, ê°„ë‹¨í•œ ì„¤ì •
**ë‹¨ì **: ì„œë²„ ë¦¬ì†ŒìŠ¤ ì¶”ê°€ ì‚¬ìš©

```bash
# PM2ë¡œ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬
npm install -g pm2

# CLIP ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
cd clip-server
pm2 start server.py --name clip-server --interpreter python3

# Next.jsì™€ í•¨ê»˜ ì‹¤í–‰
pm2 start "yarn dev" --name nextjs
pm2 save
pm2 startup
```

### Option 2: Docker ì»¨í…Œì´ë„ˆ

`clip-server/Dockerfile`:
```dockerfile
FROM python:3.10-slim

# CUDA ì§€ì› í•„ìš” ì‹œ: FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì„œë²„ ì½”ë“œ ë³µì‚¬
COPY server.py .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# ì„œë²„ ì‹¤í–‰
CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "8000"]
```

`clip-server/requirements.txt`:
```txt
torch==2.1.0
torchvision==0.16.0
transformers==4.35.0
pillow==10.1.0
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
python-multipart==0.0.6
requests==2.31.0
```

**ë¹Œë“œ ë° ì‹¤í–‰**:
```bash
cd clip-server

# CPU ë²„ì „
docker build -t clip-server .
docker run -d -p 8000:8000 --name clip-server clip-server

# GPU ë²„ì „ (NVIDIA Docker í•„ìš”)
docker build -t clip-server-gpu -f Dockerfile.gpu .
docker run -d --gpus all -p 8000:8000 --name clip-server-gpu clip-server-gpu
```

### Option 3: ë³„ë„ ì„œë²„ (ë¶„ë¦¬ ì•„í‚¤í…ì²˜)

**ì¥ì **: ë¦¬ì†ŒìŠ¤ ë¶„ì‚°, ë…ë¦½ì ì¸ ìŠ¤ì¼€ì¼ë§
**ë‹¨ì **: ë„¤íŠ¸ì›Œí¬ ì§€ì—°, ë³µì¡í•œ ì„¤ì •

```bash
# CLIP ì„œë²„ (ë³„ë„ ì„œë²„)
server1:8000

# Next.js ì„œë²„ (ê¸°ì¡´ ì„œë²„)
server2:3000
```

`.env.local`:
```bash
LOCAL_CLIP_ENDPOINT=http://server1:8000
```

### Option 4: Serverless (AWS Lambda)

**ì¥ì **: ì˜¤í† ìŠ¤ì¼€ì¼ë§, ì‚¬ìš©í•œ ë§Œí¼ë§Œ ê³¼ê¸ˆ
**ë‹¨ì **: Cold start, ë³µì¡í•œ ì„¤ì •

AWS LambdaëŠ” ëª¨ë¸ í¬ê¸° ì œí•œìœ¼ë¡œ **ê¶Œì¥í•˜ì§€ ì•ŠìŒ** (CLIP ëª¨ë¸ ~1.7GB)

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ëª¨ë¸ ì–‘ìí™” (4-bit/8-bit)

```python
from transformers import BitsAndBytesConfig

# 8-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ 50% ì ˆê°)
quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model = CLIPModel.from_pretrained(
    MODEL_NAME,
    quantization_config=quantization_config,
    device_map="auto"
)
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ í™œìš©

```typescript
// 10ê°œ ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
const clipResult = await clipFilter.filterImagesBySimilarity(
  sceneSummary,
  images,
  { topK: 10, batchSize: 10 }  // ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì§€ì •
);
```

### 3. Redis ìºì‹± ì¶”ê°€

```python
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, db=0)

@app.post("/embed")
async def create_embedding(request: EmbeddingRequest):
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"clip:{request.type}:{hash(request.input)}"

    # ìºì‹œ í™•ì¸
    cached = redis_client.get(cache_key)
    if cached:
        return EmbeddingResponse(
            success=True,
            embedding=json.loads(cached)
        )

    # ... ì„ë² ë”© ìƒì„± ...

    # ìºì‹œ ì €ì¥ (1ì‹œê°„ TTL)
    redis_client.setex(cache_key, 3600, json.dumps(embedding))

    return EmbeddingResponse(success=True, embedding=embedding)
```

---

## ë¹„ìš© ë¹„êµ

| ë°©ì‹ | ì´ˆê¸° ë¹„ìš© | ì›” ìš´ì˜ ë¹„ìš© | ì´ë¯¸ì§€ 1000ì¥ ì²˜ë¦¬ ë¹„ìš© |
|------|----------|-------------|----------------------|
| **ë¡œì»¬ ì„œë²„ (CPU)** | $0 | $0 | $0 |
| **ë¡œì»¬ ì„œë²„ (GPU)** | GPU êµ¬ë§¤ ë¹„ìš© | ì „ê¸°ë£Œ (~$10) | $0 |
| **AWS EC2 (g4dn.xlarge)** | $0 | ~$300/ì›” | $0 (ë¬´ì œí•œ) |
| **Replicate** | $0 | $0 | ~$0.10 |

**ì¶”ì²œ**: ì›” 1000íšŒ ì´ìƒ ì‚¬ìš© ì‹œ ë¡œì»¬ ì„œë²„ê°€ ìœ ë¦¬

---

## ë¬¸ì œ í•´ê²°

### CUDA out of memory

```python
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°
BATCH_SIZE = 4  # ê¸°ë³¸ê°’: 8

# ë˜ëŠ” ëª¨ë¸ ì–‘ìí™” ì‚¬ìš©
quantization_config = BitsAndBytesConfig(load_in_8bit=True)
```

### Apple Silicon (M1/M2) ì„±ëŠ¥ ì €í•˜

```python
# MPS ë°±ì—”ë“œ ëª…ì‹œì  ì„¤ì •
if torch.backends.mps.is_available():
    device = torch.device("mps")
    model = model.to(device)
```

### ëŠë¦° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ

```python
# íƒ€ì„ì•„ì›ƒ ì„¤ì •
response = requests.get(url, timeout=5)

# ë˜ëŠ” ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ ì‚¬ìš© (aiohttp)
```

---

## ë‹¤ìŒ ë‹¨ê³„

1. âœ… Python CLIP ì„œë²„ êµ¬ì¶•
2. âœ… Next.js í†µí•©
3. ğŸ”„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”
4. ğŸ”„ í”„ë¡œë•ì…˜ ë°°í¬ (PM2/Docker)
5. ğŸ”„ ëª¨ë‹ˆí„°ë§ ì„¤ì • (Prometheus/Grafana)

---

## ì°¸ê³  ìë£Œ

- [CLIP GitHub](https://github.com/openai/CLIP)
- [Hugging Face CLIP](https://huggingface.co/docs/transformers/model_doc/clip)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [PyTorch Installation](https://pytorch.org/get-started/locally/)
